# Instructions

To help guide your project, we've broken it down into a series of steps.

## Step 1: Scope the Project and Gather Data

Since the scope of the project will be highly dependent on the data, these two things happen simultaneously. In this step, youâ€™ll:

- Identify and gather the data you'll be using for your project (at least tFwo sources and more than 1 million rows). See Project Resources for ideas of what data you can use.
- Explain what end use cases you'd like to prepare the data for (e.g., analytics table, app back-end, source-of-truth database, etc.)

## Step 2: Explore and Assess the Data

- Explore the data to identify data quality issues, like missing values, duplicate data, etc.
- Document steps necessary to clean the data

## Step 3: Define the Data Model

- Map out the conceptual data model and explain why you chose that model
- List the steps necessary to pipeline the data into the chosen data model

## Step 4: Run ETL to Model the Data

- Create the data pipelines and the data model
- Include a data dictionary
- Run data quality checks to ensure the pipeline ran as expected
  - Integrity constraints on the relational database (e.g., unique key, data type, etc.)
  - Unit tests for the scripts to ensure they are doing the right thing
  - Source/count checks to ensure completeness

## Step 5: Complete Project Write Up

- What's the goal? What queries will you want to run? How would Spark or Airflow be incorporated? Why did you choose the model you chose?
- Clearly state the rationale for the choice of tools and technologies for the project.
- Document the steps of the process.
- Propose how often the data should be updated and why.
- Post your write-up and final data model in a GitHub repo.
- Include a description of how you would approach the problem differently under the following scenarios:
  - If the data was increased by 100x.
  - If the pipelines were run on a daily basis by 7am.
  - If the database needed to be accessed by 100+ people.

## Rubric

In the Project Rubric, you'll see more detail about the requirements. Use the rubric to assess your own project before you submit to Udacity for review. As with other projects, Udacity reviewers will use this rubric to assess your project and provide feedback. If your project does not meet specifications, you can make changes and resubmit.

### Write Up

| Success Criteria | Specifications |
| --- | --- |
| Scoping the Project | The write up includes an outline of the steps taken in the project. The purpose of the final data model is made explicit. |
| Addressing Other Scenarios | The write up describes a logical approach to this project under the following scenarios: The data was increased by 100x. The pipelines would be run on a daily basis by 7 am every day. The database needed to be accessed by 100+ people. |
| Defending Decisions | The choice of tools, technologies, and data model are justified well. |

### Execution

| Success Criteria | Specifications |
| --- | --- |
| Project code is clean and modular. | All coding scripts have an intuitive, easy-to-follow structure with code separated into logical functions. Naming for variables and functions follows the PEP8 style guidelines. The code should run without errors. |
| Quality Checks | The project includes at least two data quality checks. |
| Data Model | The ETL processes result in the data model outlined in the write-up. A data dictionary for the final data model is included. The data model is appropriate for the identified purpose. |
| Datasets | The project includes: At least 2 data sources More than 1 million lines of data. At least two data sources/formats (csv, api, json) |
