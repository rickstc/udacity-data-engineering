# Cloud Data Warehouses: Project

## Submission Information

Author: Timothy Ricks

Submission Date: 2023-03-04

External Repository: [GitHub](https://github.com/rickstc/udacity-data-engineering)

---

## Project Abstract

The project narrative states that the student has been tasked with assisting Sparkify, a music streaming startup, in building an ETL pipeline. This pipeline needs to extract song and log data from JSON files stored in S3, stage them in Redshift, and transform the data into a set of dimensional tables that Sparkify's analytics team can use.

---

## Documentation

The following is a list of files contained in the repository:

| File             | Description                                                                                                                                                                                                                                                                                                                                                                          |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| create_tables.py | This file contains the necessary functionality to drop and create tables using the queries defined in `sql_queries.py`. The student created a Redshift class to handle other interactions with the Redshift database, and built this functionality into that class. Therefore, this file is provided for context, and is fully functional but not necessary to run the ETL pipeline. |
| dwh.cfg-sample   | This configuration file allows the user to configure their redshift connection parameters and S3 buckets where the song data is stored. This file is a template meant to be filled in by the user, and renamed to `dwh.cfg`. |
| iac.py           | This file provides a consistent means for creating, removing, and getting the status of a Redshift cluster within AWS. |
| Pipfile and Pipefile.lock | These files define the package requirements for the project, using pipenv. |
| README.md | This file; documents the project. |
| redshift.py | A custom class for handling interactions with the Redshift cluster. This class abstracts connecting to the cluster, wraps executing queries providing a common interface, and basic error handling through logging to a file. This replaces a lot of the boilerplate code that would otherwise be necessary in the `etl` and `create_tables` files. |
| requirements.txt | Legacy requirements.txt file generated by pipenv for use when pipenv is not available. |
| sql_queries.py | The file provided by Udacity for the student to fill in to populate sql queries necessary to accomplish Sparkify's objective. Queries contained include loading the event data into Redshift staging tables, building the data tables from the staging tables, and defining the analytical queries for Sparkify's analytics team. |

---

## Quickstart

---

### Install the Dependencies

To run this application locally, the dependencies must be installed (preferably in a virtual environment).

#### Using Pipenv (preferred)

```bash
pipenv install
pipenv shell
```

#### Using Pip (not preferred)

```bash
pip install -r requirements.txt
```

---

### Configure the Environment

Rename the "dwh.cfg-sample" file to "dwh.cfg" and fill in the required configuration information.

```bash
mv dwh.cfg-sample dwh.cfg
```

    Be sure to fill in the required configuration so that the application can make a connection with your redshift cluster

### Create Redshift Cluster (optional)
    Skip this step if you already have a running Redshift cluster

The student has provided a python file that will create, check the status of, and remove a Redshift cluster in your AWS environment. Usage is as follows:

```bash
python iac.py start # Create the cluster
python iac.py # Check the status of the cluster
python iac.py stop # Remove the cluster
```

### Run the ETL Pipeline
    Important: It is not necessary to run `create_tables.py` - the ETL Pipeline will handle creating the tables all the way through running the analytic queries. This file is provided for reference purposes.

Run the ETL pipeline by using the `etl.py` file.

```bash
python etl.py
```

The following output was generated for the student:
```bash
('Dwight Yoakam', 37)
('Kid Cudi / Kanye West / Common', 10)
('Kid Cudi', 10)
('Lonnie Gordon', 9)
('Ron Carter', 9)
('B.o.B', 8)
('Usher featuring Jermaine Dupri', 6)
('Usher', 6)
('Muse', 6)
('Richard Hawley And Death Ramps_ Arctic Monkeys', 5)


("You're The One", 8)
('Catch You Baby (Steve Pitron & Max Sanna Radio Edit)', 4)
('Up Up & Away', 3)
('Long Black Road', 2)
("If I Ain't Got You", 2)
('Fade To Black', 2)
('These Words', 1)
("Let's Get It Started", 1)
('Walking On Sunshine', 1)
('The Joker', 1)


('F', 236)
('M', 97)


('Thursday ', 65)
('Monday   ', 62)
('Wednesday', 61)
('Friday   ', 53)
('Tuesday  ', 45)
('Saturday ', 31)
('Sunday   ', 16)
```
